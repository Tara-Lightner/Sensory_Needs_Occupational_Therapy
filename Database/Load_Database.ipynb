{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cf638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies.\n",
    "# config.py should contain one line with the variable db_password set to a string of your PGAdmin password, for example:\n",
    "# db_password = 'Password123'\n",
    "# Do not upload config.py to a repository.\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9bc434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing rows 0 to 10000...Done. 27.50048851966858 total seconds elapsed.\n",
      "Importing rows 10000 to 20000...Done. 64.25368309020996 total seconds elapsed.\n",
      "Importing rows 20000 to 30000...Done. 88.98372602462769 total seconds elapsed.\n",
      "Importing rows 30000 to 40000...Done. 113.85944819450378 total seconds elapsed.\n",
      "Importing rows 40000 to 50000...Done. 141.2112741470337 total seconds elapsed.\n",
      "Importing rows 50000 to 60000...Done. 178.16720509529114 total seconds elapsed.\n",
      "Importing rows 60000 to 70000...Done. 203.73585438728333 total seconds elapsed.\n",
      "Importing rows 70000 to 72210...Done. 208.7496373653412 total seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL and create a table containing the data set.\n",
    "# Import 10000 rows at a time into the database and time each set.\n",
    "# Data for balanced random forest model initial data exploration.\n",
    "\n",
    "# Database engine connection.\n",
    "db_string = f'postgresql://postgres:{db_password}@127.0.0.1:5432/Sensory_Needs_Occupational_Therapy'\n",
    "    \n",
    "# Create the database engine.\n",
    "engine = create_engine(db_string)\n",
    "    \n",
    "# Opening a connection.\n",
    "connection = engine.raw_connection()\n",
    "    \n",
    "# Creating a cursor object using the cursor() method.\n",
    "cursor = connection.cursor()\n",
    "    \n",
    "# Dropping the table if it already exists.\n",
    "cursor.execute('DROP TABLE IF EXISTS NSCH_Data_1920')\n",
    "    \n",
    "# Commit changes in the database.\n",
    "connection.commit()\n",
    "    \n",
    "# Close the connection.\n",
    "connection.close()\n",
    "    \n",
    "# Import data to SQL using chunksize parameter\n",
    "# Create a variable for the number of rows imported.\n",
    "rows_imported = 0\n",
    "    \n",
    "# Create the start time variable.\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'../data/Main_Sean_File_Numeric_2019-2020 NSCH_Topical_CAHMI_DRC_1920.csv', chunksize=10000):\n",
    "        \n",
    "    # Print the range of rows being imported.\n",
    "    print(f'Importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='NSCH_Data_1920', con=engine, if_exists='append')\n",
    "        \n",
    "    # Increase the number of rows imported by the chunksize.\n",
    "    rows_imported += len(data)\n",
    "        \n",
    "    # Print that the rows have finished importing and add the elapsed time to final print out.\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f372f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing rows 0 to 10000...Done. 60.52180504798889 total seconds elapsed.\n",
      "Importing rows 10000 to 20000...Done. 114.29114031791687 total seconds elapsed.\n",
      "Importing rows 20000 to 30000...Done. 165.6097915172577 total seconds elapsed.\n",
      "Importing rows 30000 to 40000...Done. 221.1725628376007 total seconds elapsed.\n",
      "Importing rows 40000 to 50000...Done. 275.11490654945374 total seconds elapsed.\n",
      "Importing rows 50000 to 60000...Done. 331.9701795578003 total seconds elapsed.\n",
      "Importing rows 60000 to 70000...Done. 395.9325475692749 total seconds elapsed.\n",
      "Importing rows 70000 to 72210...Done. 421.5774509906769 total seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Raw data upload.\n",
    "\n",
    "# Opening a connection.\n",
    "connection = engine.raw_connection()\n",
    "    \n",
    "# Creating a cursor object using the cursor() method.\n",
    "cursor = connection.cursor()\n",
    "    \n",
    "# Dropping the table if it already exists.\n",
    "cursor.execute('DROP TABLE IF EXISTS NSCH_Data')\n",
    "    \n",
    "# Commit changes in the database.\n",
    "connection.commit()\n",
    "    \n",
    "# Close the connection.\n",
    "connection.close()\n",
    "    \n",
    "# Import data to SQL using chunksize parameter\n",
    "# Create a variable for the number of rows imported.\n",
    "rows_imported = 0\n",
    "    \n",
    "# Create the start time variable.\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'../data/RAW_2019-2020 NSCH_Topical_CAHMI_DRC.csv', chunksize=10000):\n",
    "        \n",
    "    # Print the range of rows being imported.\n",
    "    print(f'Importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='NSCH_Data', con=engine, if_exists='append')\n",
    "        \n",
    "    # Increase the number of rows imported by the chunksize.\n",
    "    rows_imported += len(data)\n",
    "        \n",
    "    # Print that the rows have finished importing and add the elapsed time to final print out.\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d5a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing rows 0 to 10000...Done. 0.7609965801239014 total seconds elapsed.\n",
      "Importing rows 10000 to 20000...Done. 1.5810010433197021 total seconds elapsed.\n",
      "Importing rows 20000 to 30000...Done. 2.8240017890930176 total seconds elapsed.\n",
      "Importing rows 30000 to 40000...Done. 3.546996593475342 total seconds elapsed.\n",
      "Importing rows 40000 to 50000...Done. 4.280995845794678 total seconds elapsed.\n",
      "Importing rows 50000 to 60000...Done. 4.989046573638916 total seconds elapsed.\n",
      "Importing rows 60000 to 70000...Done. 5.763997554779053 total seconds elapsed.\n",
      "Importing rows 70000 to 72210...Done. 5.927463531494141 total seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Potential machine learning data subset upload.\n",
    "\n",
    "# Opening a connection.\n",
    "connection = engine.raw_connection()\n",
    "    \n",
    "# Creating a cursor object using the cursor() method.\n",
    "cursor = connection.cursor()\n",
    "    \n",
    "# Dropping the table if it already exists.\n",
    "cursor.execute('DROP TABLE IF EXISTS NSCH_Data_ML_Subset')\n",
    "    \n",
    "# Commit changes in the database.\n",
    "connection.commit()\n",
    "    \n",
    "# Close the connection.\n",
    "connection.close()\n",
    "    \n",
    "# Import data to SQL using chunksize parameter\n",
    "# Create a variable for the number of rows imported.\n",
    "rows_imported = 0\n",
    "    \n",
    "# Create the start time variable.\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'../data/NSCH_Data_ML_Subset_Sean.csv', chunksize=10000):\n",
    "        \n",
    "    # Print the range of rows being imported.\n",
    "    print(f'Importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='NSCH_Data_ML_Subset', con=engine, if_exists='append')\n",
    "        \n",
    "    # Increase the number of rows imported by the chunksize.\n",
    "    rows_imported += len(data)\n",
    "        \n",
    "    # Print that the rows have finished importing and add the elapsed time to final print out.\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70a4da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State FIPS code upload.\n",
    "\n",
    "# Opening a connection.\n",
    "connection = engine.raw_connection()\n",
    "    \n",
    "# Creating a cursor object using the cursor() method.\n",
    "cursor = connection.cursor()\n",
    "    \n",
    "# Dropping the table if it already exists.\n",
    "cursor.execute('DROP TABLE IF EXISTS State_Codes')\n",
    "    \n",
    "# Commit changes in the database.\n",
    "connection.commit()\n",
    "    \n",
    "# Close the connection.\n",
    "connection.close()\n",
    "    \n",
    "# Import data to SQL using chunksize parameter\n",
    "# Create a variable for the number of rows imported.\n",
    "rows_imported = 0\n",
    "    \n",
    "# Load state codes into database.\n",
    "data = pd.read_csv('../data/State_Codes.csv')\n",
    "\n",
    "data.to_sql(name='State_Codes', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366eeeac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
